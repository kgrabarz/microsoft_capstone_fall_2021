{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_COMPAS(temp_df, is_GAN = False):\n",
    "    \"\"\"\n",
    "    Pick columns = ['age_cat','priors_count','sex_numeric','juv_fel_count', 'juv_misd_count', \n",
    "    'juv_other_count', 'length_of_stay','race','two_year_recid', 'c_charge_degree'] \n",
    "    from the dataframe. Convert category data into numeric and remove races other \n",
    "    than Caucasian or African-American. \n",
    "    \n",
    "    Input: COMPAS dataframe\n",
    "    Output: \n",
    "           COMPAS dataframe after cleaning\n",
    "            age_cat                    int64\n",
    "            priors_count               int64\n",
    "            sex_numeric                int64\n",
    "            juv_fel_count              int64\n",
    "            juv_misd_count             int64\n",
    "            juv_other_count            int64\n",
    "            c_charge_degree_numeric    int64\n",
    "            length_of_stay             int64\n",
    "            race                       int64\n",
    "            two_year_recid             int64\n",
    "    \"\"\"\n",
    "    # deepcopy\n",
    "    df = temp_df.copy()\n",
    "    \n",
    "    # remove invalid/null entries\n",
    "    df = df[(df['days_b_screening_arrest'] <= 30)\n",
    "                & (df['days_b_screening_arrest'] >= -30)\n",
    "                & (df['is_recid'] != -1)\n",
    "                & (df['c_charge_degree'] != 'O')\n",
    "                & (df['score_text'] != 'N/A')]\n",
    "    # remove races other than Caucasian or African-American\n",
    "    df = df[(df['race']=='Caucasian') | (df['race']=='African-American')]\n",
    "\n",
    "    # calculate length_of_stay\n",
    "    df['c_jail_out'] = pd.to_datetime(df['c_jail_out'])\n",
    "    df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "    # the number of seconds to the unix epoch start\n",
    "    df['length_of_stay'] = (df['c_jail_out'] - df['c_jail_in']).astype(int) / 10**9 \n",
    "    # convert seconds into month\n",
    "    df['length_of_stay'] /= 60 * 60 * 24 * 31\n",
    "    df['length_of_stay'] = df['length_of_stay'].astype(int)\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # convert category data into numeric\n",
    "    df['sex_numeric'] = pd.factorize(df['sex'])[0]\n",
    "    df['c_charge_degree_numeric'] = pd.factorize(df['c_charge_degree'])[0]\n",
    "    race = {0: 'Caucasian', 1: 'African-American'}\n",
    "    df[\"race\"] = df[\"race\"].map({v: k for k, v in race.items()})\n",
    "    \n",
    "    if is_GAN:\n",
    "      # don't need to bin age\n",
    "      df[\"age_cat\"] = df[\"age\"]\n",
    "    else:\n",
    "      age = {1: 'Greater than 45', 0: '25 - 45', -1: 'Less than 25'}\n",
    "      df[\"age_cat\"] = df[\"age_cat\"].map({v: k for k, v in age.items()})\n",
    "\n",
    "    cols = ['age_cat','priors_count','sex_numeric','juv_fel_count', 'juv_misd_count', 'juv_other_count', 'c_charge_degree_numeric', 'length_of_stay','race','two_year_recid']\n",
    "    df = df[cols]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv'\n",
    "df = pd.read_csv(url)\n",
    "df = process_COMPAS(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split processed data into train and test\n",
    "# non_priv_data = processed_adult.drop([\"gender_labs\", \"over_under_50k\"], axis=1)\n",
    "# non_priv_train, non_priv_test = train_test_split(non_priv_data, test_size=0.2, random_state=0)\n",
    "\n",
    "# Split processed data into train and test (fixed random seed)\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=0)\n",
    "non_priv_train = train_data\n",
    "\n",
    "# Split test data into X and y\n",
    "x_test, y_test = test_data.drop([\"two_year_recid\",\"race\"], axis=1), test_data[\"two_year_recid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age_cat                    int64\n",
      "priors_count               int64\n",
      "sex_numeric                int64\n",
      "juv_fel_count              int64\n",
      "juv_misd_count             int64\n",
      "juv_other_count            int64\n",
      "c_charge_degree_numeric    int64\n",
      "length_of_stay             int64\n",
      "race                       int64\n",
      "two_year_recid             int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compas_classification_helper(synthesizer, eps, rep, classifier, test_df, non_priv_results=False):\n",
    "    \"\"\" helper function to calculate TPR_diff, FPR_diff, and F1_score for a single synthesizer, eps, rep, and classifier \"\"\"\n",
    "    \n",
    "    # Read in the synthetic training data or use non-private data\n",
    "    if non_priv_results:\n",
    "        train_df = non_priv_train\n",
    "    else:\n",
    "        fname = f\"{synthesizer}/COMPAS_{synthesizer}_eps={eps}_{rep}.csv\" # typo: todo: need to fix this back to Compas\n",
    "        train_df = pd.read_csv(fname, index_col=0)\n",
    "\n",
    "    # Train the classification model\n",
    "    X_train, y_train = train_df.drop([\"two_year_recid\",\"race\"], axis=1), train_df[\"two_year_recid\"]\n",
    "    X_test, y_test = test_df.drop([\"two_year_recid\",\"race\"], axis=1), test_df[\"two_year_recid\"]\n",
    "#     X_train, y_train = train_df.drop(\"label\", axis=1), train_df[\"label\"]\n",
    "#     X_test, y_test = test_df.drop(\"label\", axis=1), test_df[\"label\"]\n",
    "    if len(np.unique(y_train)) != 2:\n",
    "        return None\n",
    "    if classifier == \"logistic\":\n",
    "        m = LogisticRegression(max_iter=1000, C=1., penalty=\"l2\")\n",
    "        m.fit(X_train, y_train)\n",
    "    elif classifier == \"forest\":\n",
    "        m = RandomForestClassifier(random_state=0)\n",
    "        m.fit(X_train, y_train)\n",
    "    else:\n",
    "        raise Exception(f\"Classifier {classifier} not recognized.\")\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_test_pred = m.predict(X_test)\n",
    "\n",
    "    # Get the all classification metrics on the test set\n",
    "    df = test_df.copy() # test_df\n",
    "    df[\"y\"] = y_test\n",
    "    df[\"y_pred\"] = y_test_pred\n",
    "#     print(df.dtypes)\n",
    "    TP_f = len(df[(df[\"race\"]==0) & (df[\"y\"]==1) & (df[\"y_pred\"]==1)])\n",
    "    TP_m = len(df[(df[\"race\"]==1) & (df[\"y\"]==1) & (df[\"y_pred\"]==1)])\n",
    "    FP_f = len(df[(df[\"race\"]==0) & (df[\"y\"]==0) & (df[\"y_pred\"]==1)])\n",
    "    FP_m = len(df[(df[\"race\"]==1) & (df[\"y\"]==0) & (df[\"y_pred\"]==1)])\n",
    "    TN_f = len(df[(df[\"race\"]==0) & (df[\"y\"]==0) & (df[\"y_pred\"]==0)])\n",
    "    TN_m = len(df[(df[\"race\"]==1) & (df[\"y\"]==0) & (df[\"y_pred\"]==0)])\n",
    "    FN_f = len(df[(df[\"race\"]==0) & (df[\"y\"]==1) & (df[\"y_pred\"]==0)])\n",
    "    FN_m = len(df[(df[\"race\"]==1) & (df[\"y\"]==1) & (df[\"y_pred\"]==0)])\n",
    "    TPR_f = TP_f / (TP_f + FN_f)\n",
    "    TPR_m = TP_m / (TP_m + FN_m)\n",
    "    FPR_f = FP_f / (FP_f + TN_f)\n",
    "    FPR_m = FP_m / (FP_m + TN_m)\n",
    "    TPR_diff = TPR_m - TPR_f\n",
    "    FPR_diff = FPR_m - FPR_f\n",
    "    f1_score = (TP_f+TP_m) / (TP_f+TP_m + 0.5*(FP_f+FP_m + FN_f+FN_m))\n",
    "    \n",
    "    return (TPR_diff, FPR_diff, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compas_table_metrics(synthesizer, epsilon_list, nreps, classifier, test_df, f1_metric=\"median\"):\n",
    "    \"\"\" return median TPR_diff, FPR_diff, F1_score across all epsilons and repetitions \"\"\"\n",
    "    \n",
    "    # Initialize lists\n",
    "    tpr_diff_list = []\n",
    "    fpr_diff_list = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    # Loop through the epsilon values and repetitions\n",
    "    for eps in [3.0]:\n",
    "        for rep in range(nreps):\n",
    "\n",
    "            # Get the classification metrics\n",
    "            results = compas_classification_helper(synthesizer, eps, rep, classifier, test_df)\n",
    "            if results is not None:\n",
    "                TPR_diff, FPR_diff, f1_score = results\n",
    "            \n",
    "                # Append metrics to lists\n",
    "                tpr_diff_list.append(TPR_diff)\n",
    "                fpr_diff_list.append(FPR_diff)\n",
    "                f1_scores.append(f1_score)\n",
    "            \n",
    "    # Get medians\n",
    "    tpr_diff_median = np.median(tpr_diff_list)\n",
    "    fpr_diff_median = np.median(fpr_diff_list)\n",
    "    if f1_metric == \"median\":\n",
    "        f1_score_metric = np.median(f1_scores)\n",
    "    elif f1_metric == \"max\":\n",
    "        f1_score_metric = np.max(f1_scores)\n",
    "    \n",
    "    return tpr_diff_median, fpr_diff_median, f1_score_metric\n",
    "\n",
    "def get_compas_plot_metrics(synthesizer, epsilon_list, nreps, classifier, test_df, f1_metric=\"median\"):\n",
    "    \"\"\" return median and std of TPR_diff, FPR_diff, F1_score for *each* epsilon value (arrays of values) \"\"\"\n",
    "    \n",
    "    # Initialize lists\n",
    "    tpr_diff_median_list = []\n",
    "    fpr_diff_median_list = []\n",
    "    f1_score_metrics = []\n",
    "    tpr_diff_std_list = []\n",
    "    fpr_diff_std_list = []\n",
    "    f1_score_std_list = []\n",
    "    \n",
    "    # Loop through the epsilon values and repetitions\n",
    "    for eps in epsilon_list:\n",
    "        \n",
    "        # Initialize list to hold values for each epsilon value\n",
    "        tpr_diff_list = []\n",
    "        fpr_diff_list = []\n",
    "        f1_scores = []\n",
    "        \n",
    "        # Loop through the range of repetitions\n",
    "        for rep in range(nreps):\n",
    "\n",
    "            # Get the classification metrics\n",
    "            results = compas_classification_helper(synthesizer, eps, rep, classifier, test_df)\n",
    "            if results is not None:\n",
    "                TPR_diff, FPR_diff, f1_score = results\n",
    "            \n",
    "                # Append metrics to lists\n",
    "                tpr_diff_list.append(TPR_diff)\n",
    "                fpr_diff_list.append(FPR_diff)\n",
    "                f1_scores.append(f1_score)\n",
    "            \n",
    "        # Append medians to lists\n",
    "        tpr_diff_median_list.append(np.median(tpr_diff_list))\n",
    "        fpr_diff_median_list.append(np.median(fpr_diff_list))\n",
    "        if f1_metric == \"median\":\n",
    "            f1_score_metrics.append(np.median(f1_scores))\n",
    "        elif f1_metric == \"max\":\n",
    "            f1_score_metrics.append(np.max(f1_scores))\n",
    "        tpr_diff_std_list.append(np.std(tpr_diff_list))\n",
    "        fpr_diff_std_list.append(np.std(fpr_diff_list))\n",
    "        f1_score_std_list.append(np.std(f1_scores))\n",
    "        \n",
    "    return tpr_diff_median_list, fpr_diff_median_list, f1_score_metrics, tpr_diff_std_list, fpr_diff_std_list, f1_score_std_list\n",
    "        \n",
    "def get_compas_epsilon_plots(synthesizer_list, epsilon_list, nreps, classifier, test_df, f1_metric=\"median\", non_priv_results=True):\n",
    "    \"\"\" return subplot with three plots showing graphs of TPR_diff, FPR_diff, F1_score acros epsilons for each synthesizer \"\"\"\n",
    "    \n",
    "    # Initialize subplots\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(16,5))\n",
    "    \n",
    "    # Loop through the synthesizers\n",
    "    for synth in synthesizer_list:\n",
    "        \n",
    "        # Get all classification metrics\n",
    "        tpr_diff_median_list, fpr_diff_median_list, f1_score_metrics, tpr_diff_std_list, fpr_diff_std_list, f1_score_std_list \\\n",
    "        =  get_compas_plot_metrics(synth, epsilon_list, nreps, classifier, test_df, f1_metric)\n",
    "        \n",
    "        # Plot the metrics with error bars\n",
    "        ax[0].errorbar(epsilon_list, tpr_diff_median_list, tpr_diff_std_list, label=synth)\n",
    "        ax[1].errorbar(epsilon_list, fpr_diff_median_list, fpr_diff_std_list, label=synth)\n",
    "        ax[2].errorbar(epsilon_list, f1_score_metrics, f1_score_std_list, label=synth)\n",
    "        \n",
    "    # Add the non-private results to the plots\n",
    "    non_priv_tpr_diff, non_priv_fpr_diff, non_priv_f1_score = compas_classification_helper(None, None, None, classifier, test_df, non_priv_results=True)\n",
    "    ax[0].hlines(non_priv_tpr_diff, xmin=epsilon_list[0], xmax=epsilon_list[-1], linestyles=\"--\", label=\"Non-private data\")\n",
    "    ax[1].hlines(non_priv_fpr_diff, xmin=epsilon_list[0], xmax=epsilon_list[-1], linestyles=\"--\", label=\"Non-private data\")\n",
    "    ax[2].hlines(non_priv_f1_score, xmin=epsilon_list[0], xmax=epsilon_list[-1], linestyles=\"--\", label=\"Non-private data\")\n",
    "        \n",
    "    # Plotting details\n",
    "    for i in range(3):\n",
    "        ax[i].set_xlabel(\"Privacy budget\")\n",
    "        ax[i].legend()\n",
    "    ax[0].set_ylabel(\"Equalized odds distance ($y=1$)\")\n",
    "    ax[1].set_ylabel(\"Equalized odds distance ($y=0$)\")\n",
    "    ax[2].set_ylabel(\"F1-score\")\n",
    "    if classifier == \"logistic\":\n",
    "        title = f\"Compas classification summary (logistic regression)\"\n",
    "    elif classifier == \"forest\":\n",
    "        title = f\"Compas classification summary (random forest)\"\n",
    "    fig.suptitle(title, size=20)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = np.linspace(0.5,5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MWEM\n",
      "Logistic Regression:\n",
      "TPR difference median: 0.011054\n",
      "FPR difference median: 0.023593\n",
      "F1-score median: 0.176962\n",
      "Random Forest:\n",
      "TPR difference median: 0.017734\n",
      "FPR difference median: 0.021885\n",
      "F1-score median: 0.439294\n",
      "\n",
      "MWEMQUAIL\n",
      "Logistic Regression:\n",
      "TPR difference median: 0.227484\n",
      "FPR difference median: 0.164097\n",
      "F1-score median: 0.547131\n",
      "Random Forest:\n",
      "TPR difference median: 0.196787\n",
      "FPR difference median: 0.144856\n",
      "F1-score median: 0.543897\n",
      "\n",
      "DPCTGAN\n",
      "Logistic Regression:\n",
      "TPR difference median: 0.000000\n",
      "FPR difference median: 0.000000\n",
      "F1-score median: 0.604125\n",
      "Random Forest:\n",
      "TPR difference median: 0.000000\n",
      "FPR difference median: 0.000000\n",
      "F1-score median: 0.609788\n",
      "\n",
      "PATECTGAN\n",
      "Logistic Regression:\n",
      "TPR difference median: -0.003300\n",
      "FPR difference median: 0.000000\n",
      "F1-score median: 0.614173\n",
      "Random Forest:\n",
      "TPR difference median: -0.028411\n",
      "FPR difference median: 0.006550\n",
      "F1-score median: 0.516640\n",
      "\n",
      "DPCTGANQUAIL\n",
      "Logistic Regression:\n",
      "TPR difference median: 0.151636\n",
      "FPR difference median: 0.096301\n",
      "F1-score median: 0.539349\n",
      "Random Forest:\n",
      "TPR difference median: 0.160006\n",
      "FPR difference median: 0.088441\n",
      "F1-score median: 0.545633\n",
      "\n",
      "PATECTGANQUAIL\n",
      "Logistic Regression:\n",
      "TPR difference median: 0.203229\n",
      "FPR difference median: 0.066299\n",
      "F1-score median: 0.553911\n",
      "Random Forest:\n",
      "TPR difference median: 0.177085\n",
      "FPR difference median: 0.082008\n",
      "F1-score median: 0.531959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for synthesizer in [\"MWEM\", \"MWEMQUAIL\", \"DPCTGAN\", \"PATECTGAN\", \"DPCTGANQUAIL\", \"PATECTGANQUAIL\"]:\n",
    "    print(synthesizer)\n",
    "    for classifier, name in zip([\"logistic\", \"forest\"], [\"Logistic Regression\", \"Random Forest\"]):\n",
    "        print(name + \":\")\n",
    "        tpr_diff_median, fpr_diff_median, f1_score_median = get_compas_table_metrics(synthesizer, epsilon_list=epsilons, nreps=15, classifier=classifier, test_df=test_data)\n",
    "        print(f\"TPR difference median: {tpr_diff_median:f}\")\n",
    "        print(f\"FPR difference median: {fpr_diff_median:f}\")\n",
    "        print(f\"F1-score median: {f1_score_median:f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset\n",
      "Logistic Regression:\n",
      "TPR difference median: 0.338642\n",
      "FPR difference median: 0.171700\n",
      "F1-score: 0.566411\n",
      "Random Forest:\n",
      "TPR difference median: 0.283848\n",
      "FPR difference median: 0.119157\n",
      "F1-score: 0.580858\n"
     ]
    }
   ],
   "source": [
    "# non-private data\n",
    "print(\"Original Dataset\")\n",
    "print(\"Logistic Regression:\")\n",
    "non_priv_tpr_diff, non_priv_fpr_diff, non_priv_f1_score = compas_classification_helper(\"\", [], 0, \"logistic\", test_data, non_priv_results=True)\n",
    "print(f\"TPR difference median: {non_priv_tpr_diff:f}\")\n",
    "print(f\"FPR difference median: {non_priv_fpr_diff:f}\")\n",
    "print(f\"F1-score: {non_priv_f1_score:f}\")\n",
    "\n",
    "print(\"Random Forest:\")\n",
    "non_priv_tpr_diff, non_priv_fpr_diff, non_priv_f1_score = compas_classification_helper(\"\", [], 0, \"forest\", test_data, non_priv_results=True)\n",
    "print(f\"TPR difference median: {non_priv_tpr_diff:f}\")\n",
    "print(f\"FPR difference median: {non_priv_fpr_diff:f}\")\n",
    "print(f\"F1-score: {non_priv_f1_score:f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
